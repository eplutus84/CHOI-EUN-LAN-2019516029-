{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwccvXSVo1Zu"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAK8Vk7NpwmZ"
   },
   "outputs": [],
   "source": [
    "!unzip en_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ckegeK-Opw4J"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import Text\n",
    "import collections\n",
    "from keras.layers.core import Dense, SpatialDropout1D \n",
    "from keras.layers.convolutional import Conv1D \n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import np_utils \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "676MPkolo658"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words('english'))\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "with open('friends_train.json') as json_file:\n",
    "    json_train = json.load(json_file)\n",
    "with open('friends_test.json') as json_file:\n",
    "    json_test = json.load(json_file)\n",
    "with open('friends_dev.json') as json_file:\n",
    "    json_dev = json.load(json_file)\n",
    "\n",
    "def cleaning(str):\n",
    "    replaceAll= str\n",
    "    only_english = re.sub('[^a-zA-Z]', ' ', replaceAll)\n",
    "    no_capitals = only_english.lower().split()\n",
    "    no_stops = [word for word in no_capitals if not word in stops]\n",
    "    stemmer_words = [stemmer.stem(word) for word in no_stops]\n",
    "    return ' '.join(stemmer_words)\n",
    "\n",
    "i = 0\n",
    "train_data=[]\n",
    "for rows in json_train:\n",
    "    for row in rows:\n",
    "        train_data.append([cleaning(row['utterance']), row['emotion']])\n",
    "for rows in json_test:\n",
    "    for row in rows:\n",
    "        train_data.append([cleaning(row['utterance']), row['emotion']])\n",
    "for rows in json_dev:\n",
    "    for row in rows:\n",
    "        train_data.append([cleaning(row['utterance']), row['emotion']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwhTdhX_o6-Y"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "cnt = 0\n",
    "tagged = []\n",
    "counter = collections.Counter()\n",
    "for d in train_data:\n",
    "    cnt = cnt + 1\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt)\n",
    "    words = pos_tag(word_tokenize(d[0]))\n",
    "    for t in words:\n",
    "        word = \"/\".join(t)\n",
    "        tagged.append(word)\n",
    "        counter[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPbHEM-Co7Bu"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "word2index = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    word2index[word[0]] = wid + 1\n",
    "vocab_sz = len(word2index) + 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcf3BlReo7Ey"
   },
   "outputs": [],
   "source": [
    "def labeltoint(str):\n",
    "    return {'non-neutral': 0,'neutral': 1, 'joy': 2,'sadness': 3,'fear': 4,'anger': 5,'surprise': 6,'disgust': 7}[str]\n",
    "\n",
    "xs, ys = [], []\n",
    "cnt = 0\n",
    "maxlen = 0\n",
    "for d in train_data:\n",
    "    cnt = cnt + 1\n",
    "    ys.append(labeltoint(d[1]))\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt)\n",
    "    ang = pos_tag(word_tokenize(d[0]))\n",
    "    words=[]\n",
    "    for t in ang:\n",
    "        words.append(\"/\".join(t))\n",
    "    if len(words) > maxlen: \n",
    "        maxlen = len(words)\n",
    "    wids = [word2index[word] for word in words]\n",
    "    xs.append(wids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfDUhTvDo7Hp"
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(xs, maxlen=maxlen) \n",
    "Y = np_utils.to_categorical(ys)\n",
    " \n",
    "EMBED_SIZE = 100 \n",
    "NUM_FILTERS = 256 \n",
    "NUM_WORDS = 3 \n",
    "BATCH_SIZE = 64 \n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "model = Sequential() \n",
    "model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen)) \n",
    "model.add(SpatialDropout1D(0.2)) \n",
    "#model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\")) \n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \n",
    "model.add(Dense(8, activation=\"softmax\")) \n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(x_test, y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtKLN4fpo7Kt"
   },
   "outputs": [],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_YKsDUSo7Nw"
   },
   "outputs": [],
   "source": [
    "def inttolabel(idx):\n",
    "    return {0:'non-neutral',\n",
    "             1:'neutral', \n",
    "             2:'joy',\n",
    "             3:'sadness',\n",
    "             4:'fear',\n",
    "             5:'anger',\n",
    "             6:'surprise',\n",
    "             7:'disgust'}[idx]\n",
    "\n",
    "def predict(text): \n",
    "    aa = pos_tag(word_tokenize(text))\n",
    "    pp = []\n",
    "    for t in aa:\n",
    "        pp.append(\"/\".join(t))\n",
    "    wids = [word2index[word] for word in pp]\n",
    "    x_predict = pad_sequences([wids], maxlen=maxlen) \n",
    "    y_predict = model.predict(x_predict) \n",
    "    c = 0\n",
    "    cnt = 0\n",
    "    for y in y_predict[0]:\n",
    "        if c < y:\n",
    "            c = y\n",
    "            ans = cnt\n",
    "        cnt += 1\n",
    "    ans = inttolabel(ans)\n",
    "    return ans;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ewUagwLz6qD"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JR6fjAbo7R4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('en_data.csv', 'r', newline='') as csvfile:\n",
    "    df = pd.read_csv(csvfile)\n",
    "cnt = 0\n",
    "dap = []\n",
    "for i in df['utterance']:\n",
    "    cnt+=1\n",
    "    if cnt % 1000 == 0 : \n",
    "        print(cnt)\n",
    "    dap.append(predict(i))\n",
    "result = [['Id','Expected']]\n",
    "cnt = -1\n",
    "\n",
    "for i in dap:\n",
    "    cnt += 1\n",
    "    result.append([cnt, i])\n",
    "dataframe = pd.DataFrame(result)\n",
    "dataframe.to_csv(\"/content/drive/MyDrive/Colab Notebooks/Subfriend lstm.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N46UwJX9o7Uz"
   },
   "outputs": [],
   "source": [
    "# for real test\n",
    "\n",
    "ans = predict('i love it')\n",
    "print(ans)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Friends(LSTM).ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
